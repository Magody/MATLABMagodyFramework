- Check adam, maybe we should accumulate the gradient until reach the end of epoch.
- Optimice CNN -> vectorice backward for Convolutional and Pooling layers, and add vectorization when using stride > 1 for both.
- Include Leaky relu activation function.
- Add K-fold.
- Set history information to detect overfitting, first of all use a validation set in normal Neural Network.
- Create purelin activation -> is this different from final ouput or its not necessary to create this function?
- Add GPU operations optimization. Priorizing reusability of code.
- Create experiment and excel writer. Reusable.
- Models are heavy in MB, optimice that with function to store only trully necessary values.
- Remove unnecesary code, refactor.
- Include a function to compute numerical gradient, to test that gradient is actually correct.
- parametrizar los valores de Adam
- epsilon and alpha decay should decay by inner epoch or episode? Move and parametice according to.
- implement ResNet residual blocks
- CNN: el backward usaba un mean de los usuarios, eso se hace correctamente?
- CNN: el gradiente está equivocado por un poco y la actualización de los mesos no considera el promedio
- controlar la property "t" de Dense y otras layers, está creciendo mucho.
- la softmax es una capa de activación y la función de pérdida espera logits en lugar de una activación
- check numerical gradient with true gradient obtained
- implement computational graph: ver el grafo computacional en https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html, usando el grafo computacional almacena varias cosas que aceleran un monton el procesamiento para no recalcular dos veces
